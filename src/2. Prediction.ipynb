{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df594f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_PATH = \"../data/PROCESSED/ndvi.csv\"\n",
    "PROD_PATH = \"../data/PROCESSED/manhuacu.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Hyperparameters\n",
    "MLP_BATCH_SIZE = 16\n",
    "MLP_WINDOW_SIZE = 10\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "LSTM_WINDOW_SIZE = 10\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_NUM_LAYERS = 1\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_EPOCHS = 400\n",
    "LSTM_BATCH_SIZE = 16\n",
    "\n",
    "# Computation\n",
    "LSTM_DROPOUT = LSTM_DROPOUT if LSTM_NUM_LAYERS > 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_year_index(date: datetime):\n",
    "    \"\"\"Convert date to day of year.\"\"\"\n",
    "    return datetime(date.year, date.month, date.day).timetuple().tm_yday - 1\n",
    "\n",
    "\n",
    "def get_sin_cos(x: float):\n",
    "    \"\"\"Convert x to sin and cos.\"\"\"\n",
    "    rad = 2 * np.pi * x\n",
    "    return (np.sin(rad), np.cos(rad))\n",
    "\n",
    "\n",
    "def encode_date(date: datetime):\n",
    "    is_leap_year = 1 if date.year % 4 == 0 else 0\n",
    "    total_year_days = 366 if is_leap_year else 365\n",
    "\n",
    "    day_index = get_day_of_year_index(date)\n",
    "    \n",
    "    frac = day_index / total_year_days\n",
    "    return get_sin_cos(frac)\n",
    "\n",
    "# Test\n",
    "print(\"Encoding date 2020-01-01\")\n",
    "print(encode_date(datetime(2020, 1, 1)))  # (0.0, 1.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-06-01\")\n",
    "print(encode_date(datetime(2020, 6, 1)))  # (0.5, 0.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-12-31\")\n",
    "print(encode_date(datetime(2020, 12, 31)))  # (0.9999999999999999, 1.0)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ca232",
   "metadata": {},
   "source": [
    "## 1. Carregar e Pré-processar Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fba421",
   "metadata": {},
   "source": [
    "### 1.1. Carregar e pre-processar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI = pd.read_csv(NDVI_PATH)\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NDVI[\"N_Observations\"] = NDVI.groupby(\"Year\")[\"Data\"].transform(\"count\")\n",
    "\n",
    "NDVI[[\"Date_sin\", \"Date_cos\"]] = NDVI[\"Data\"].apply(\n",
    "    lambda x: pd.Series(encode_date(datetime.strptime(x, \"%Y-%m-%d\")))\n",
    ")\n",
    "\n",
    "# Assert order by Data (ascending)\n",
    "NDVI = NDVI.sort_values(by=\"Data\", ascending=True)\n",
    "\n",
    "NDVI = NDVI[(NDVI[\"Year\"] >= 2000) & (NDVI[\"Year\"] <= 2023)]\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = pd.read_csv(PROD_PATH)\n",
    "PROD = PROD[(PROD[\"Year\"] >= 2000) & (PROD[\"Year\"] <= 2023)]\n",
    "# max_productivity = PROD[\"Productivity (kg/ha)\"].max()\n",
    "# PROD[\"Normalized_productivity\"] = PROD[\"Productivity (kg/ha)\"] / max_productivity\n",
    "PROD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba00741",
   "metadata": {},
   "source": [
    "### 1.2. Visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.plot(x=\"Data\", y=\"NDVI\", title=\"NDVI over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be13ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLPDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, ndvi_df, prod_df):\n",
    "#         self.ndvi_df = ndvi_df\n",
    "#         self.prod_df = prod_df\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.ndvi_df[\"Year\"].nunique()\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         years = self.ndvi_df[\"Year\"].sort_values().unique()\n",
    "#         if idx >= len(years):\n",
    "#             raise IndexError(\"Index out of range\")\n",
    "#         year = years[idx]\n",
    "#         ndvi = self.ndvi_df[self.ndvi_df[\"Year\"] == year][\"NDVI\"].values\n",
    "#         prod = self.prod_df[self.prod_df[\"Year\"] == year][\n",
    "#             \"Productivity (kg/ha)\"\n",
    "#         ].values[0]\n",
    "#         return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "#             prod, dtype=torch.float32\n",
    "#         )\n",
    "\n",
    "\n",
    "# dataset = MLPDataset(NDVI_last_20_per_year, PROD)\n",
    "# dataset[0]\n",
    "\n",
    "# train_size = len(dataset) - 8\n",
    "# valid_size = 4\n",
    "# test_size = 4\n",
    "# train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
    "# valid_dateset = torch.utils.data.Subset(dataset, range(train_size, train_size + valid_size))\n",
    "# test_dataset = torch.utils.data.Subset(dataset, range(train_size + valid_size, train_size + valid_size + test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb013d",
   "metadata": {},
   "source": [
    "### 2.2. Preparar Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbdd8",
   "metadata": {},
   "source": [
    "#### 2.1.1. Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizer dados NDVI\n",
    "\n",
    "NDVI[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "ndvi_scaler = StandardScaler().fit(NDVI[[\"NDVI\", \"Year\"]].values)\n",
    "NDVI[[\"NDVI_norm\", \"Year_norm\"]] = ndvi_scaler.transform(\n",
    "    NDVI[[\"NDVI\", \"Year\"]].values\n",
    ")\n",
    "\n",
    "NDVI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd382cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar produtividade\n",
    "PROD[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "prod_scaler = StandardScaler().fit(PROD[[\"Productivity (kg/ha)\", \"Year\"]].values)\n",
    "PROD[[\"Productivity_norm\", \"Year_norm\"]] = prod_scaler.transform(\n",
    "    PROD[[\"Productivity (kg/ha)\", \"Year\"]].values\n",
    ")\n",
    "PROD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetYearOfLast(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    DatasetYearOfLast - Dataset para previsão de produtividade\n",
    "\n",
    "    X: Sequências de tamanho <WINDOW_SIZE> de observações de NDVI consecutivas (normalizado -1 a +1)\n",
    "    y: Produtividade no Ano da última observação\n",
    "\n",
    "    Features:\n",
    "    - Sequências de NDVI (Já vem normalizado entre 0 e 1 da fonte)\n",
    "    - Sequências de dia do ano com codificação circular no formato de Tupla: (Seno, Cosseno)\n",
    "    - Sequência de ano da observação normalizado por z-score\n",
    "\n",
    "    Label:\n",
    "    - Produtividade (kg/ha) normalizada por z-score, relativa ao ano da última observação\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df\n",
    "        self.prod_df = prod_df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ndvi_df) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ndvi = self.ndvi_df.iloc[idx : idx + self.window_size][\n",
    "            [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "        ].values\n",
    "\n",
    "        year = self.ndvi_df.iloc[idx + self.window_size - 1][\"Year\"]\n",
    "        prod = self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0]\n",
    "\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "train_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"] <= 2016], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[(NDVI[\"Year\"] > 2016) & (NDVI[\"Year\"] <= 2020)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "test_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"] > 2020], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "print(validation_dataset_year_of_last[0][0].shape)\n",
    "validation_dataset_year_of_last[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ae9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWeightedAverage(torch.utils.data.Dataset):\n",
    "    \"\"\"DatasetWeightedAverage - Dataset para previsão de produtividade\n",
    "\n",
    "    X: Sequências de tamanho <WINDOW_SIZE> de observações de NDVI consecutivas (normalizado -1 a +1)\n",
    "    y: Produtividade média ponderada entre a produtividade do ano da primeira observação e do ano da última observação\n",
    "    - (normalizado por z-score)\n",
    "    - A média é ponderada pela quantidade de observações do ano da primeira e do ano da última observação\n",
    "\n",
    "    Features:\n",
    "    - Sequências de NDVI (Já vem normalizado entre 0 e 1 da fonte)\n",
    "    - Sequências de dia do ano com codificação circular no formato de Tupla: (Seno, Cosseno)\n",
    "    - Sequência de ano da observação normalizado por z-score\n",
    "\n",
    "    Label:\n",
    "    - Produtividade (kg/ha) média ponderada entre o ano da primeira e do ano da última observação, normalizada por z-score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df\n",
    "        self.prod_df = prod_df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ndvi_df) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ndvi = self.ndvi_df.iloc[idx : idx + self.window_size][\n",
    "            [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "        ].values\n",
    "\n",
    "        year_first = self.ndvi_df.iloc[idx][\"Year\"]\n",
    "        year_last = self.ndvi_df.iloc[idx + self.window_size][\"Year\"]\n",
    "\n",
    "        prod_first = self.prod_df[self.prod_df[\"Year\"] == year_first][\n",
    "            \"Productivity_norm\"\n",
    "        ].values[0]\n",
    "        prod_last = self.prod_df[self.prod_df[\"Year\"] == year_last][\n",
    "            \"Productivity_norm\"\n",
    "        ].values[0]\n",
    "\n",
    "        n_obs_first = self.ndvi_df.iloc[idx : idx + self.window_size].loc[\n",
    "            self.ndvi_df.iloc[idx : idx + self.window_size][\"Year\"] == year_first\n",
    "        ].shape[0]\n",
    "        n_obs_last = self.ndvi_df.iloc[idx : idx + self.window_size].loc[\n",
    "            self.ndvi_df[\"Year\"] == year_last\n",
    "        ].shape[0]\n",
    "\n",
    "        # Ponderação\n",
    "        prod = (n_obs_first * prod_first + n_obs_last * prod_last) / (\n",
    "            n_obs_first + n_obs_last\n",
    "        )\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "# Test Dataset\n",
    "train_dataset_weighted_average = DatasetWeightedAverage(NDVI[NDVI[\"Year\"] <= 2016], PROD, LSTM_WINDOW_SIZE)\n",
    "validation_dataset_weighted_average = DatasetWeightedAverage(NDVI[(NDVI[\"Year\"] > 2016) & (NDVI[\"Year\"] <= 2020)], PROD, LSTM_WINDOW_SIZE)\n",
    "test_dataset_weighted_average = DatasetWeightedAverage(NDVI[NDVI[\"Year\"] > 2020], PROD, LSTM_WINDOW_SIZE)\n",
    "\n",
    "validation_dataset_weighted_average[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b975",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62edc0",
   "metadata": {},
   "source": [
    "### 3.1. Multi-layer Perceptron\n",
    "\n",
    "Essa rede é uma feedforward perceptron multi-layer comum (1 camada interna).\n",
    "\n",
    "As entradas são os 20 últimos NDVIs do ano, a saída é a produtividade prevista (kg/ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4442be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(MLP_WINDOW_SIZE * 4, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "\n",
    "def init_linear_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.normal_(m.bias, mean=0.0, std=0.01)\n",
    "\n",
    "mlp_network.apply(init_linear_weights)\n",
    "\n",
    "# for name, param in mlp_network.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n",
    "\n",
    "# Step-by-step debug the MLP\n",
    "# x = torch.randn(20, 4)\n",
    "# print(f\"Input shape: {x.shape}\\n{x}\\n\")\n",
    "# for i, layer in enumerate(mlp_network):\n",
    "#     x = layer(x)\n",
    "#     print(f\"After layer {i} ({layer.__class__.__name__}): {x.shape}\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = mlp_network.to(device)\n",
    "optimizer = optim.Adam(mlp_network.parameters(), lr=5e-6)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "mlp_losses = []\n",
    "best_loss = float(\"inf\")\n",
    "saved_epoch = 0\n",
    "\n",
    "train_loader_weighted_average = torch.utils.data.DataLoader(\n",
    "    train_dataset_weighted_average, batch_size=MLP_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "validation_loader_weighted_average = torch.utils.data.DataLoader(\n",
    "    validation_dataset_weighted_average, batch_size=4, shuffle=True\n",
    ")\n",
    "\n",
    "for i in trange(400):\n",
    "    mlp_network.train()\n",
    "    for ndvi, prod in train_loader_weighted_average:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = mlp_network(ndvi)\n",
    "        loss = loss_fn(pred.squeeze(dim=1), prod)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_losses = []\n",
    "    mlp_network.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in validation_loader_weighted_average:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = mlp_network(ndvi)\n",
    "            loss = loss_fn(pred.squeeze(dim=1), prod)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        if np.mean(epoch_losses) < best_loss:\n",
    "            best_loss = np.mean(epoch_losses)\n",
    "            saved_epoch = i + 1\n",
    "            torch.save(mlp_network.state_dict(), \"mlp.pth\")\n",
    "\n",
    "    mlp_losses.append(np.mean(epoch_losses))\n",
    "print(f\"\\n\\nSaved MLP model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs.\n",
    "\n",
    "    Args:\n",
    "        losses (list): List of loss values for each epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(mlp_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bbc47",
   "metadata": {},
   "source": [
    "### 3.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define model with Linear layer\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=LSTM_DROPOUT\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, x, hidden_n=None, hidden_c=None):\n",
    "        if hidden_n is None or hidden_c is None:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out[:, -1, :])  # Get output of last time step\n",
    "        else:\n",
    "            out, (hidden_n, hidden_c) = self.lstm(x, (hidden_n, hidden_c))\n",
    "            out = self.fc(out[:, -1, :])  # Get output of last time step\n",
    "            return out, (hidden_n, hidden_c)\n",
    "\n",
    "\n",
    "train_loader_weighted_average = torch.utils.data.DataLoader(\n",
    "    train_dataset_weighted_average,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "validation_loader_weighted_average = torch.utils.data.DataLoader(\n",
    "    validation_dataset_weighted_average,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "\n",
    "lstm_model = LSTMRegressor(\n",
    "    input_size=4, hidden_size=LSTM_HIDDEN_SIZE, num_layers=LSTM_NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "# for name, param in lstm_model.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n",
    "\n",
    "\n",
    "def init_lstm_weights(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        nn.init.xavier_uniform_(m.weight_ih_l0)\n",
    "        nn.init.xavier_uniform_(m.weight_hh_l0)\n",
    "\n",
    "\n",
    "lstm_model.apply(init_lstm_weights)\n",
    "lstm_model.apply(init_linear_weights)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=5e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "lstm_losses = []\n",
    "best_loss = float(\"inf\")\n",
    "saved_epoch = 0\n",
    "\n",
    "\n",
    "for i in range(LSTM_EPOCHS):\n",
    "    h_n = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "        device\n",
    "    )  # Hidden state\n",
    "    h_c = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "        device\n",
    "    )  # Cell state\n",
    "\n",
    "    lstm_model.train()\n",
    "    for ndvi, prod in train_loader_weighted_average:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred, (h_n, h_c) = lstm_model(\n",
    "            ndvi, h_n.detach(), h_c.detach()\n",
    "        )\n",
    "        last_pred = pred[:, -1]\n",
    "        loss = loss_fn(last_pred, prod)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_losses = []\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in validation_loader_weighted_average:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = lstm_model(ndvi)\n",
    "            last_pred = pred[:, -1]  # Get the last prediction\n",
    "            loss = loss_fn(last_pred, prod)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            saved_epoch = i + 1\n",
    "            torch.save(lstm_model.state_dict(), \"lstm.pth\")\n",
    "\n",
    "    # if (i) % 10 == 0:\n",
    "    print(f\"Epoch {i+1}/{LSTM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    lstm_losses.append(avg_loss)\n",
    "\n",
    "print(f\"\\n\\nSaved LSTM model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(lstm_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae30be1",
   "metadata": {},
   "source": [
    "## 4. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_weighted_average = torch.utils.data.DataLoader(\n",
    "    test_dataset_weighted_average, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "losses_mlp, pred_mlp = [], []\n",
    "losses_lstm, pred_lstm = [], []\n",
    "for ndvi, prod in test_loader_weighted_average:\n",
    "    for model, losses_arr, preds_arr in zip(\n",
    "        [mlp_network, lstm_model], [losses_mlp, losses_lstm], [pred_mlp, pred_lstm]\n",
    "    ):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = model(ndvi)\n",
    "            last_pred = pred[:, -1]  # Get the last prediction\n",
    "            loss = loss_fn(last_pred, prod)\n",
    "            losses_arr.append(loss.item())\n",
    "            preds_arr.append(last_pred.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe713cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a398718",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe81a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_weighted_average[0]\n",
    "\n",
    "seq = []\n",
    "for i in range(len(test_dataset_weighted_average)):\n",
    "    _, prod = test_dataset_weighted_average[i]\n",
    "    # print(f\"Prod z-score: {prod}\")\n",
    "    # print(\"Predicted MLP z-score: \", pred_mlp[i])\n",
    "    # print(\"Predicted LSTM z-score: \", pred_lstm[i])\n",
    "    seq.append(\n",
    "        {\n",
    "            \"Prod\": prod.item(),\n",
    "            \"Pred_MLP\": pred_mlp[i],\n",
    "            \"Pred_LSTM\": pred_lstm[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(seq)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999adf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    figsize=(12, 6),\n",
    "    y=[\"Prod\", \"Pred_MLP\", \"Pred_LSTM\"],\n",
    "    title=\"Comparison of Prod, Pred_MLP, and Pred_LSTM\",\n",
    "    xlabel=\"Index\",\n",
    "    ylabel=\"Values\",\n",
    "    grid=True,\n",
    ")\n",
    "plt.legend([\"Prod\", \"Pred_MLP\", \"Pred_LSTM\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
