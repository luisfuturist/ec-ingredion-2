{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from typing import List, Tuple, Dict\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization, acquisition\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# Ensure deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df594f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_PATH = \"../data/PROCESSED/ndvi.csv\"\n",
    "PROD_PATH = \"../data/PROCESSED/manhuacu.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Hyperparameters\n",
    "MLP_WINDOW_SIZE = 10\n",
    "MLP_BATCH_SIZE = 8\n",
    "MLP_BASE_HIDDEN_SIZE = 64\n",
    "MLP_EPOCHS = 400\n",
    "MLP_LEARNING_RATE = 1e-3\n",
    "MLP_DROPOUT = 0.2\n",
    "\n",
    "# Best MLP Hyperparameters\n",
    "# MLP_WINDOW_SIZE = 10\n",
    "# MLP_BATCH_SIZE = 4\n",
    "# MLP_BASE_HIDDEN_SIZE = 32\n",
    "# MLP_EPOCHS = 300\n",
    "# MLP_LEARNING_RATE = 2e-3\n",
    "\n",
    "# Validation Loss: 0.1911\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "LSTM_WINDOW_SIZE = 10\n",
    "LSTM_HIDDEN_SIZE = 32\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_EPOCHS = 1000\n",
    "LSTM_BATCH_SIZE = 4\n",
    "LSTM_LEARNING_RATE = 1e-4\n",
    "\n",
    "# Best LSTM Hyperparameters So Far\n",
    "# LSTM_WINDOW_SIZE = 20\n",
    "# LSTM_HIDDEN_SIZE = 32\n",
    "# LSTM_NUM_LAYERS = 2\n",
    "# LSTM_DROPOUT = 0.2\n",
    "# LSTM_EPOCHS = 1000\n",
    "# LSTM_BATCH_SIZE = 16\n",
    "# LSTM_LEARNING_RATE = 5e-5\n",
    "\n",
    "# Validation Loss: 0.2122\n",
    "\n",
    "# Bayesian Optimization Hyperparameters\n",
    "INIT_POINTS = 3\n",
    "N_ITER = 15\n",
    "\n",
    "\n",
    "# Computation\n",
    "LSTM_DROPOUT = LSTM_DROPOUT if LSTM_NUM_LAYERS > 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_year_index(date: datetime):\n",
    "    \"\"\"Convert date to day of year.\"\"\"\n",
    "    return datetime(date.year, date.month, date.day).timetuple().tm_yday - 1\n",
    "\n",
    "\n",
    "def get_sin_cos(x: float):\n",
    "    \"\"\"Convert x to sin and cos.\"\"\"\n",
    "    rad = 2 * np.pi * x\n",
    "    return (np.sin(rad), np.cos(rad))\n",
    "\n",
    "\n",
    "def encode_date(date: datetime):\n",
    "    is_leap_year = 1 if date.year % 4 == 0 else 0\n",
    "    total_year_days = 366 if is_leap_year else 365\n",
    "    day_index = get_day_of_year_index(date)\n",
    "    return get_sin_cos(day_index / total_year_days)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Encoding date 2020-01-01\")\n",
    "print(encode_date(datetime(2020, 1, 1)))  # (0.0, 1.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-06-01\")\n",
    "print(encode_date(datetime(2020, 6, 1)))  # (0.5, 0.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-12-31\")\n",
    "print(encode_date(datetime(2020, 12, 31)))  # (0.9999999999999999, 1.0)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ca232",
   "metadata": {},
   "source": [
    "## 1. Carregar e Pré-processar Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fba421",
   "metadata": {},
   "source": [
    "### 1.1. Carregar e pre-processar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI = pd.read_csv(NDVI_PATH)\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI[\"N_Observations\"] = NDVI.groupby(\"Year\")[\"Data\"].transform(\"count\")\n",
    "\n",
    "NDVI[[\"Date_sin\", \"Date_cos\"]] = NDVI[\"Data\"].apply(\n",
    "    lambda x: pd.Series(encode_date(datetime.strptime(x, \"%Y-%m-%d\")))\n",
    ")\n",
    "\n",
    "# Assert order by Data (ascending)\n",
    "NDVI = NDVI.sort_values(by=\"Data\", ascending=True)\n",
    "\n",
    "NDVI = NDVI[(NDVI[\"Year\"] >= 2000) & (NDVI[\"Year\"] <= 2023)]\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = pd.read_csv(PROD_PATH)\n",
    "PROD = PROD[(PROD[\"Year\"] >= 2000) & (PROD[\"Year\"] <= 2023)]\n",
    "# max_productivity = PROD[\"Productivity (kg/ha)\"].max()\n",
    "# PROD[\"Normalized_productivity\"] = PROD[\"Productivity (kg/ha)\"] / max_productivity\n",
    "PROD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba00741",
   "metadata": {},
   "source": [
    "### 1.2. Visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.plot(x=\"Data\", y=\"NDVI\", title=\"NDVI over time\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb013d",
   "metadata": {},
   "source": [
    "### 2.2. Preparar Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbdd8",
   "metadata": {},
   "source": [
    "#### 2.1.1. Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizer dados NDVI\n",
    "NDVI[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "ndvi_scaler = StandardScaler().fit(NDVI[[\"NDVI\", \"Year\"]].values)\n",
    "NDVI[[\"NDVI_norm\", \"Year_norm\"]] = ndvi_scaler.transform(NDVI[[\"NDVI\", \"Year\"]].values)\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd382cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar produtividade\n",
    "PROD[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "prod_scaler = StandardScaler().fit(PROD[[\"Productivity (kg/ha)\", \"Year\"]].values)\n",
    "PROD[[\"Productivity_norm\", \"Year_norm\"]] = prod_scaler.transform(\n",
    "    PROD[[\"Productivity (kg/ha)\", \"Year\"]].values\n",
    ")\n",
    "PROD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetYearOfLast(torch.utils.data.Dataset):\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df.copy().reset_index(drop=True)\n",
    "        self.window_size = window_size\n",
    "        self.prod_df = prod_df\n",
    "\n",
    "        # Prepare windows grouped by year\n",
    "        self.samples = []\n",
    "        self.available_years = ndvi_df[\"Year\"].unique().tolist()\n",
    "\n",
    "        for idx, row in self.ndvi_df.iterrows():\n",
    "            window = ndvi_df.iloc[idx : idx + window_size]\n",
    "\n",
    "            if len(window) < window_size:\n",
    "                break\n",
    "\n",
    "            last_of_window = self.ndvi_df.iloc[idx + window_size - 1]\n",
    "\n",
    "            if last_of_window[\"Year\"] == row[\"Year\"] or (\n",
    "                last_of_window[\"Year\"] == row[\"Year\"] + 1\n",
    "                and row[\"Year\"] + 1 in self.available_years\n",
    "            ):\n",
    "                self.samples.append((window, last_of_window[\"Year\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window, year = self.samples[idx]\n",
    "        ndvi = window[[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]].values\n",
    "\n",
    "        prod = self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0]\n",
    "\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def get_last_window_of_year(self, year) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retorna a última janela do ano\n",
    "        \"\"\"\n",
    "\n",
    "        ndvi = self.ndvi_df[self.ndvi_df[\"Year\"] == year]\n",
    "        if ndvi.empty:\n",
    "            raise ValueError(f\"Year {year} not found in dataset\")\n",
    "        if len(ndvi) < self.window_size:\n",
    "            raise ValueError(\n",
    "                f\"Year {year} has only {len(ndvi)} observations, less than window size {self.window_size}\"\n",
    "            )\n",
    "        return (\n",
    "            ndvi.iloc[-self.window_size :][\n",
    "                [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "            ].values,\n",
    "            self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0],\n",
    "        )\n",
    "\n",
    "\n",
    "years_validation = [2004, 2010, 2016, 2022]\n",
    "years_test = [2005, 2011, 2017, 2023]\n",
    "years_train = PROD[~PROD[\"Year\"].isin(years_validation + years_test)][\"Year\"].unique()\n",
    "\n",
    "\n",
    "# Datasets better\n",
    "dataset_train = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_train)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "dataset_validation = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "mlp_test_dataset = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "lstm_train_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_train)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "lstm_validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "lstm_test_dataset = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "print(dataset_validation[0])\n",
    "print(\"\\n\")\n",
    "print(dataset_validation.get_last_window_of_year(2004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a26fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"Last window of 2017 in the DataFrame:\")\n",
    "print(\n",
    "    NDVI[NDVI[\"Year\"] == 2017][[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]].tail(\n",
    "        LSTM_WINDOW_SIZE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nProdutivity 2017: {PROD[PROD['Year'] == 2017]['Productivity (kg/ha)'].values[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Produtivity 2017 (normalized): {PROD[PROD['Year'] == 2017]['Productivity_norm'].values[0]}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nLast window of 2017 in the DatasetYearOfLast (values should match exactely):\")\n",
    "print(lstm_test_dataset.get_last_window_of_year(2017))\n",
    "\n",
    "assert (\n",
    "    lstm_test_dataset.get_last_window_of_year(2017)[0]\n",
    "    == NDVI[NDVI[\"Year\"] == 2017][[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]]\n",
    "    .tail(LSTM_WINDOW_SIZE)\n",
    "    .values\n",
    ").all(), \"\\n❌ Sanity check failed! Please check the DatasetYearOfLast class\"\n",
    "print(\n",
    "    \"\\n✅ Sanity check passed for LSTM! You can look values by yourself if you want to double check.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b975",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62edc0",
   "metadata": {},
   "source": [
    "### 3.1. Multi-layer Perceptron\n",
    "\n",
    "Essa rede é uma feedforward perceptron multi-layer comum (1 camada interna).\n",
    "\n",
    "As entradas são os 20 últimos NDVIs do ano, a saída é a produtividade prevista (kg/ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4442be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmplifiedTanh(nn.Module):\n",
    "    def __init__(self, amplification_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.amplification_factor = amplification_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.amplification_factor * torch.tanh(x)\n",
    "\n",
    "\n",
    "def load_mlp_datasets(window_size=MLP_WINDOW_SIZE):\n",
    "    dataset_train = DatasetYearOfLast(\n",
    "        NDVI[NDVI[\"Year\"].isin(years_train)], PROD, window_size\n",
    "    )\n",
    "    dataset_validation = DatasetYearOfLast(\n",
    "        NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, window_size\n",
    "    )\n",
    "    return (\n",
    "        dataset_train,\n",
    "        dataset_validation,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mlp_loaders(\n",
    "    dataset_train: DatasetYearOfLast,\n",
    "    dataset_validation: DatasetYearOfLast,\n",
    "    batch_size=MLP_BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Load MLP datasets and return DataLoader objects.\"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset_validation, batch_size=batch_size, shuffle=False, drop_last=True\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def init_linear_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def init_mlp_network(\n",
    "    base_hidden_size=MLP_BASE_HIDDEN_SIZE,\n",
    "    window_size=MLP_WINDOW_SIZE,\n",
    "    dropout=MLP_DROPOUT,\n",
    "    init_weights=True,\n",
    "):\n",
    "    \"\"\"Initialize MLP network with given base hidden size and window size.\"\"\"\n",
    "    mlp_network = nn.Sequential(\n",
    "        nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        nn.Linear(window_size * 4, base_hidden_size),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(base_hidden_size, base_hidden_size // 2),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(base_hidden_size // 2, 1),\n",
    "        AmplifiedTanh(amplification_factor=1.5),\n",
    "    )\n",
    "\n",
    "    init_weights and mlp_network.apply(init_linear_weights)\n",
    "\n",
    "    return mlp_network\n",
    "\n",
    "\n",
    "# for name, param in mlp_network.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n",
    "\n",
    "# Step-by-step debug the MLP\n",
    "# x = torch.randn(20, 4)\n",
    "# print(f\"Input shape: {x.shape}\\n{x}\\n\")\n",
    "# for i, layer in enumerate(mlp_network):\n",
    "#     x = layer(x)\n",
    "#     print(f\"After layer {i} ({layer.__class__.__name__}): {x.shape}\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_model = None\n",
    "best_mlp_loss = float(\"inf\")\n",
    "best_mlp_losses_train = []\n",
    "best_mlp_losses_validation = []\n",
    "\n",
    "\n",
    "def train_mlp(\n",
    "    epochs: int = MLP_EPOCHS,\n",
    "    batch_size: int = MLP_BATCH_SIZE,\n",
    "    learning_rate: float = MLP_LEARNING_RATE,\n",
    "    window_size: int = MLP_WINDOW_SIZE,\n",
    "    base_hidden_size: int = MLP_BASE_HIDDEN_SIZE,\n",
    "    dropout: float = MLP_DROPOUT,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    best_model_found = False\n",
    "    # torch.manual_seed(1)\n",
    "    global best_mlp_model\n",
    "    global best_mlp_loss\n",
    "    global best_mlp_losses_train\n",
    "    global best_mlp_losses_validation\n",
    "    train_ds, validation_ds = load_mlp_datasets(window_size=window_size)\n",
    "    train_loader, validation_loader = load_mlp_loaders(\n",
    "        train_ds, validation_ds, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    mlp_network = init_mlp_network(\n",
    "        base_hidden_size=base_hidden_size, window_size=window_size, dropout=dropout\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(mlp_network.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    mlp_losses_validation = []\n",
    "    mlp_losses_train = []\n",
    "    best_loss = float(\"inf\")\n",
    "    saved_epoch = 0\n",
    "\n",
    "    range_fn = trange if verbose else range\n",
    "    for i in range_fn(epochs):\n",
    "        epoch_losses_train = []\n",
    "\n",
    "        mlp_network.train()\n",
    "        for ndvi, prod in train_loader:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = mlp_network(ndvi)\n",
    "            loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses_train.append(loss.item())\n",
    "\n",
    "        epoch_losses_validation = []\n",
    "        mlp_network.eval()\n",
    "        with torch.no_grad():\n",
    "            for ndvi, prod in validation_loader:\n",
    "                ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "                pred = mlp_network(ndvi)\n",
    "                loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "                epoch_losses_validation.append(loss.item())\n",
    "\n",
    "            if np.mean(epoch_losses_validation) < best_loss:\n",
    "                best_loss = np.mean(epoch_losses_validation)\n",
    "                saved_epoch = i + 1\n",
    "                torch.save(mlp_network.state_dict(), \"mlp.pth\")\n",
    "\n",
    "            if np.mean(epoch_losses_validation) < best_mlp_loss:\n",
    "                best_model_found = True\n",
    "                best_mlp_model = deepcopy(mlp_network)\n",
    "                best_mlp_loss = np.mean(epoch_losses_validation)\n",
    "\n",
    "        mlp_losses_train.append(np.mean(epoch_losses_train))\n",
    "        mlp_losses_validation.append(np.mean(epoch_losses_validation))\n",
    "\n",
    "    if best_model_found:\n",
    "        best_mlp_losses_train = mlp_losses_train.copy()\n",
    "        best_mlp_losses_validation = mlp_losses_validation.copy()\n",
    "\n",
    "    verbose and print(\n",
    "        f\"\\n\\nSaved MLP model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    mlp_network.load_state_dict(torch.load(\"mlp.pth\"))\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def get_best_mlp_loss(**kwargs):\n",
    "    \"\"\"\n",
    "    Get the best loss from training\n",
    "    \"\"\"\n",
    "    return -train_mlp(\n",
    "        # epochs=int(kwargs[\"epochs\"]),\n",
    "        batch_size=int(kwargs[\"batch_size\"]),\n",
    "        learning_rate=kwargs[\"learning_rate\"],\n",
    "        window_size=int(kwargs[\"window_size\"]),\n",
    "        base_hidden_size=int(kwargs[\"base_hidden_size\"]),\n",
    "        dropout=kwargs[\"dropout\"],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "\n",
    "parameter_bounds = {\n",
    "    # \"epochs\": (200, 600),\n",
    "    \"batch_size\": (1, 12.999999999),\n",
    "    \"learning_rate\": (1e-5, 1e-2),\n",
    "    \"window_size\": (5, 20.999999999),\n",
    "    \"base_hidden_size\": (16, 128.999999999),\n",
    "    \"dropout\": (0.0, 0.3),\n",
    "}\n",
    "\n",
    "# Practical rule of thumb:\n",
    "# kappa ≈ 1 → balanced between exploration and exploitation\n",
    "# kappa < 1 → focus more on exploiting what you already know\n",
    "# kappa > 2 → emphasize exploration heavily\n",
    "acquisition_function = acquisition.UpperConfidenceBound(\n",
    "    kappa=1.5,  # Start with slight exploration\n",
    "    exploration_decay=0.76,  # Reduce kappa by 5% per step\n",
    "    exploration_decay_delay=2,  # Only start decay after 2 steps\n",
    ")\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=get_best_mlp_loss,\n",
    "    pbounds=parameter_bounds,\n",
    "    acquisition_function=acquisition_function,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=INIT_POINTS,\n",
    "    n_iter=N_ITER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_params = {**optimizer.max[\"params\"]}\n",
    "print(f\"\\n\\nBest MLP parameters:\")\n",
    "pprint(best_mlp_params)\n",
    "\n",
    "torch.save(best_mlp_model.state_dict(), \"mlp.pth\")\n",
    "best_mlp_model = best_mlp_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, validation_losses):\n",
    "    \"\"\"\n",
    "    Plots the training and validation losses over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training loss values for each epoch.\n",
    "        validation_losses (list): List of validation loss values for each epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss(best_mlp_losses_train, best_mlp_losses_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bbc47",
   "metadata": {},
   "source": [
    "### 3.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "def cat_name_generator(gender: str): return random.choice({\"macho\": [\"Thor\", \"Loki\", \"Zeus\", \"Hades\", \"Apolo\", \"Ares\", \"Hermes\", \"Poseidon\", \"Hércules\", \"Aquiles\", \"Ulisses\", \"Atlas\", \"Perseu\", \"Orfeu\", \"Eros\", \"Hefesto\", \"Dionísio\", \"Héracles\", \"Cronos\", \"Prometeu\", \"Teseu\", \"Orfeu\", \"Eolo\"], \"femea\": [\"Afrodite\", \"Artemis\", \"Deméter\", \"Hera\", \"Perséfone\", \"Atena\", \"Héstia\", \"Eris\", \"Selene\", \"Gaia\", \"Tétis\", \"Eurídice\", \"Calipso\", \"Medusa\", \"Circe\"]}[gender])\n",
    "\n",
    "print(\"Nome de gato macho:\", cat_name_generator(\"macho\"))\n",
    "print(\"Nome de gato fêmea:\", cat_name_generator(\"femea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define model with Linear layer\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=LSTM_DROPOUT):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, x, hidden_n=None, hidden_c=None):\n",
    "        if hidden_n is None or hidden_c is None:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out[:, -1, :])  # Get output of last time step\n",
    "        else:\n",
    "            out, (hidden_n, hidden_c) = self.lstm(x, (hidden_n, hidden_c))\n",
    "            out = self.fc(out[:, -1, :])  # Get output of last time step\n",
    "            return out, (hidden_n, hidden_c)\n",
    "\n",
    "\n",
    "def get_lstm_datasets(window_size=LSTM_WINDOW_SIZE):\n",
    "    lstm_train_dataset_year_of_last = DatasetYearOfLast(\n",
    "        NDVI[NDVI[\"Year\"].isin(years_train)], PROD, window_size\n",
    "    )\n",
    "    lstm_validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "        NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, window_size\n",
    "    )\n",
    "    return (\n",
    "        lstm_train_dataset_year_of_last,\n",
    "        lstm_validation_dataset_year_of_last,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lstm_dataloaders(\n",
    "    train_ds: DatasetYearOfLast,\n",
    "    validation_ds: DatasetYearOfLast,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "):\n",
    "    lstm_train_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    lstm_validation_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "        validation_ds,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return lstm_train_loader_year_of_last, lstm_validation_loader_year_of_last\n",
    "\n",
    "\n",
    "def init_lstm_weights(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def init_lstm(\n",
    "    hidden_size=LSTM_HIDDEN_SIZE, num_layers=LSTM_NUM_LAYERS, dropout=LSTM_DROPOUT\n",
    "):\n",
    "    lstm_model = LSTMRegressor(\n",
    "        input_size=4, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    lstm_model.apply(init_lstm_weights)\n",
    "    lstm_model.apply(init_linear_weights)\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "best_lstm_model = None\n",
    "best_lstm_loss = float(\"inf\")\n",
    "best_lstm_losses_train = []\n",
    "best_lstm_losses_validation = []\n",
    "\n",
    "\n",
    "def train_lstm(\n",
    "    epochs=LSTM_EPOCHS,\n",
    "    hidden_size=LSTM_HIDDEN_SIZE,\n",
    "    num_layers=LSTM_NUM_LAYERS,\n",
    "    learning_rate=LSTM_LEARNING_RATE,\n",
    "    dropout=LSTM_DROPOUT,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    window_size=LSTM_WINDOW_SIZE,\n",
    "    verbose=False,\n",
    "):\n",
    "    best_model_found = False\n",
    "    global best_lstm_model\n",
    "    global best_lstm_loss\n",
    "    global best_lstm_losses_train\n",
    "    global best_lstm_losses_validation\n",
    "\n",
    "    train_ds, validation_ds = get_lstm_datasets(window_size=window_size)\n",
    "\n",
    "    lstm_train_loader, lstm_validation_loader = get_lstm_dataloaders(\n",
    "        train_ds=train_ds, validation_ds=validation_ds, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    lstm_model = init_lstm(\n",
    "        num_layers=num_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=0.0 if num_layers == 1 else dropout,\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    lstm_losses_train = []\n",
    "    lstm_losses_validation = []\n",
    "    best_loss = float(\"inf\")\n",
    "    saved_epoch = 0\n",
    "\n",
    "    it = trange if verbose else range\n",
    "    for i in it(epochs):\n",
    "        epoch_losses_train = []\n",
    "\n",
    "        # h_n = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "        #     device\n",
    "        # )  # Hidden state\n",
    "        # h_c = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "        #     device\n",
    "        # )  # Cell state\n",
    "\n",
    "        lstm_model.train()\n",
    "        for ndvi, prod in lstm_train_loader:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # pred, (h_n, h_c) = lstm_model(\n",
    "            #     ndvi, h_n.detach(), h_c.detach()\n",
    "            # )\n",
    "\n",
    "            # Verificar se isto está certo\n",
    "            # last_pred está correto?\n",
    "            pred = lstm_model(ndvi)\n",
    "            last_pred = pred[:, -1]\n",
    "            loss = loss_fn(last_pred, prod)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            epoch_losses_train.append(loss.item())\n",
    "\n",
    "        epoch_losses_validation = []\n",
    "        lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for ndvi, prod in lstm_validation_loader:\n",
    "                ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "                pred = lstm_model(ndvi)\n",
    "                last_pred = pred[:, -1]  # Get the last prediction\n",
    "                loss = loss_fn(last_pred, prod)\n",
    "                epoch_losses_validation.append(loss.item())\n",
    "\n",
    "            if np.mean(epoch_losses_validation) < best_loss:\n",
    "                best_loss = np.mean(epoch_losses_validation)\n",
    "                saved_epoch = i + 1\n",
    "                torch.save(lstm_model.state_dict(), \"lstm.pth\")\n",
    "\n",
    "            if np.mean(epoch_losses_validation) < best_lstm_loss:\n",
    "                best_model_found = True\n",
    "                best_lstm_model = deepcopy(lstm_model)\n",
    "                best_lstm_loss = np.mean(epoch_losses_validation)\n",
    "\n",
    "        lstm_losses_train.append(np.mean(epoch_losses_train))\n",
    "        lstm_losses_validation.append(np.mean(epoch_losses_validation))\n",
    "\n",
    "    if best_model_found:\n",
    "        best_lstm_losses_train = lstm_losses_train.copy()\n",
    "        best_lstm_losses_validation = lstm_losses_validation.copy()\n",
    "\n",
    "    verbose and print(\n",
    "        f\"\\n\\nSaved LSTM model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    lstm_model.load_state_dict(torch.load(\"lstm.pth\"))\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def get_best_loss(**kwargs):\n",
    "    \"\"\"\n",
    "    Get the best loss from training\n",
    "    \"\"\"\n",
    "    return -train_lstm(\n",
    "        # epochs=int(kwargs[\"epochs\"]),\n",
    "        hidden_size=int(kwargs[\"hidden_size\"]),\n",
    "        num_layers=int(kwargs[\"num_layers\"]),\n",
    "        learning_rate=kwargs[\"learning_rate\"],\n",
    "        dropout=kwargs[\"dropout\"],\n",
    "        batch_size=int(kwargs[\"batch_size\"]),\n",
    "        window_size=int(kwargs[\"window_size\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "parameter_bounds = {\n",
    "    \"hidden_size\": (16, 128.9999999999),  # 16 to 128\n",
    "    \"num_layers\": (1, 2.9999999999),  # 1 or 2\n",
    "    \"learning_rate\": (1e-6, 1e-3),\n",
    "    \"batch_size\": (1, 16.9999999999),\n",
    "    \"dropout\": (0.0, 0.3),\n",
    "    # \"epochs\": (200, 1000.9999999999),  # (200, 1000.9999999999) 200 to 1000\n",
    "    \"window_size\": (4, 20.9999999999),  # 4 to 20\n",
    "}\n",
    "\n",
    "acquisition_function = acquisition.UpperConfidenceBound(\n",
    "    kappa=1.5,  # Start with slight exploration\n",
    "    exploration_decay=0.76,  # Reduce kappa by 5% per step\n",
    "    exploration_decay_delay=2,  # Only start decay after 2 steps\n",
    ")\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=get_best_loss,\n",
    "    pbounds=parameter_bounds,\n",
    "    acquisition_function=acquisition_function,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=INIT_POINTS,\n",
    "    n_iter=N_ITER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc510ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_params = {**optimizer.max[\"params\"]}\n",
    "print(f\"\\n\\nBest LSTM parameters:\")\n",
    "pprint(best_lstm_params)\n",
    "\n",
    "best_lstm_model = best_lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(best_lstm_losses_train, best_lstm_losses_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae30be1",
   "metadata": {},
   "source": [
    "## 4. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_dataset = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, int(best_mlp_params[\"window_size\"])\n",
    ")\n",
    "lstm_test_dataset = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, int(best_lstm_params[\"window_size\"])\n",
    ")\n",
    "\n",
    "mlp_test_loader = torch.utils.data.DataLoader(\n",
    "    mlp_test_dataset, batch_size=4, shuffle=False, drop_last=True\n",
    ")\n",
    "lstm_test_loader = torch.utils.data.DataLoader(\n",
    "    lstm_test_dataset, batch_size=4, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# test MLP\n",
    "test_losses_mlp = []\n",
    "best_mlp_model.eval()\n",
    "loss_fn = nn.MSELoss()\n",
    "for ndvi, prod in mlp_test_loader:\n",
    "    ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "    pred = best_mlp_model(ndvi)\n",
    "    loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "    test_losses_mlp.append(loss.item())\n",
    "print(f\"MLP test loss: {np.mean(test_losses_mlp):.4f}\")\n",
    "\n",
    "# test LSTM\n",
    "test_losses_lstm = []\n",
    "best_lstm_model.eval()\n",
    "for ndvi, prod in lstm_test_loader:\n",
    "    ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "    pred = best_lstm_model(ndvi)\n",
    "    last_pred = pred[:, -1]  # Get the last prediction\n",
    "    loss = loss_fn(last_pred, prod)\n",
    "    test_losses_lstm.append(loss.item())\n",
    "    \n",
    "print(f\"LSTM test loss: {np.mean(test_losses_lstm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_model.eval()\n",
    "all_preds = []\n",
    "real_prods = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ndvi, prod in mlp_test_loader:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        pred = best_mlp_model(ndvi)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        real_prods.append(prod.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and real productions\n",
    "all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "real_prods = np.concatenate(real_prods, axis=0).flatten()\n",
    "\n",
    "# Print predictions and real productions\n",
    "# print(\"LSTM Predictions for all test dataset:\")\n",
    "# print(all_preds)\n",
    "# print(\"\\nReal Production (normalized) for all test dataset:\")\n",
    "# print(real_prods)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(real_prods, label=\"Real Production (normalized)\", marker=\"o\")\n",
    "plt.plot(all_preds, label=\"MLP Predictions (normalized)\", marker=\"x\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Normalized Values\")\n",
    "plt.title(\"MLP Predictions vs Real Production\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56981a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model.eval()\n",
    "all_preds = []\n",
    "real_prods = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ndvi, prod in lstm_test_loader:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        pred = best_lstm_model(ndvi)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        real_prods.append(prod.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and real productions\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "real_prods = np.concatenate(real_prods, axis=0)\n",
    "\n",
    "# Print predictions and real productions\n",
    "# print(\"LSTM Predictions for all test dataset:\")\n",
    "# print(all_preds)\n",
    "# print(\"\\nReal Production (normalized) for all test dataset:\")\n",
    "# print(real_prods)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(real_prods, label=\"Real Production (normalized)\", marker=\"o\")\n",
    "plt.plot(all_preds, label=\"LSTM Predictions (normalized)\", marker=\"x\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Normalized Values\")\n",
    "plt.title(\"LSTM Predictions vs Real Production\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP test loss:\", np.mean(test_losses_mlp))\n",
    "for y_test in years_test:\n",
    "    ndvi, prod = mlp_test_dataset.get_last_window_of_year(y_test)\n",
    "    ndvi = torch.tensor(ndvi, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = best_mlp_model(ndvi)\n",
    "    last_pred = pred[:, -1].cpu().detach().numpy()[0]\n",
    "    test_losses_mlp.append(last_pred)\n",
    "    print(f\"\\n[MLP] Predicted (normalized) productivity for {y_test}: {last_pred:.4f}\")\n",
    "    print(f\"[MLP] Real productivity (normalized) for {y_test}: {prod:.4f}\")\n",
    "\n",
    "print(\"\\n\\nLSTM test loss:\", np.mean(test_losses_lstm))\n",
    "for y_test in years_test:\n",
    "    ndvi, prod = lstm_test_dataset.get_last_window_of_year(y_test)\n",
    "    ndvi = torch.tensor(ndvi, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = best_lstm_model(ndvi)\n",
    "    last_pred = pred[:, -1].cpu().detach().numpy()[0]\n",
    "    print(f\"\\n[LSTM] Predicted productivity (normalized) for {y_test}: {last_pred:.4f}\")\n",
    "    print(f\"[LSTM] Real productivity (normalized) for {y_test}: {prod:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
