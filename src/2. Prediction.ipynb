{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df594f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_PATH = \"../data/PROCESSED/ndvi.csv\"\n",
    "PROD_PATH = \"../data/PROCESSED/manhuacu.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Hyperparameters\n",
    "LSTM_WINDOW_SIZE = 20\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_year_index(date: datetime):\n",
    "    \"\"\"Convert date to day of year.\"\"\"\n",
    "    return datetime(date.year, date.month, date.day).timetuple().tm_yday - 1\n",
    "\n",
    "\n",
    "def get_sin_cos(x: float):\n",
    "    \"\"\"Convert x to sin and cos.\"\"\"\n",
    "    rad = 2 * np.pi * x\n",
    "    return (np.sin(rad), np.cos(rad))\n",
    "\n",
    "\n",
    "def encode_date(date: datetime):\n",
    "    is_leap_year = 1 if date.year % 4 == 0 else 0\n",
    "    total_year_days = 366 if is_leap_year else 365\n",
    "\n",
    "    day_index = get_day_of_year_index(date)\n",
    "    \n",
    "    frac = day_index / total_year_days\n",
    "    return get_sin_cos(frac)\n",
    "\n",
    "# Test\n",
    "print(\"Encoding date 2020-01-01\")\n",
    "print(encode_date(datetime(2020, 1, 1)))  # (0.0, 1.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-06-01\")\n",
    "print(encode_date(datetime(2020, 6, 1)))  # (0.5, 0.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-12-31\")\n",
    "print(encode_date(datetime(2020, 12, 31)))  # (0.9999999999999999, 1.0)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ca232",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NDVI = pd.read_csv(NDVI_PATH)\n",
    "\n",
    "NDVI[\"N_Observations\"] = NDVI.groupby(\"Year\")[\"Data\"].transform(\"count\")\n",
    "\n",
    "NDVI[[\"Date_sin\", \"Date_cos\"]] = NDVI[\"Data\"].apply(\n",
    "    lambda x: pd.Series(encode_date(datetime.strptime(x, \"%Y-%m-%d\")))\n",
    ")\n",
    "\n",
    "# Assert order by Data (ascending)\n",
    "NDVI = NDVI.sort_values(by=\"Data\", ascending=True)\n",
    "\n",
    "NDVI = NDVI[(NDVI[\"Year\"] >= 2000) & (NDVI[\"Year\"] <= 2024)]\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = pd.read_csv(PROD_PATH)\n",
    "PROD = PROD[(PROD[\"Year\"] >= 2000) & (PROD[\"Year\"] <= 2024)]\n",
    "max_productivity = PROD[\"Productivity (kg/ha)\"].max()\n",
    "PROD[\"Normalized_productivity\"] = PROD[\"Productivity (kg/ha)\"] / max_productivity\n",
    "PROD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2990c40",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_last_20_per_year = NDVI.groupby(\"Year\").tail(20)\n",
    "NDVI_last_20_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting NDVI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    NDVI_last_20_per_year[\"Data\"],\n",
    "    NDVI_last_20_per_year[\"NDVI\"],\n",
    "    label=\"NDVI\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plt.title(\"NDVI over the last 20 days of each year\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.xticks(\n",
    "    ticks=NDVI_last_20_per_year.groupby(\"Year\").tail(1).index,\n",
    "    labels=NDVI_last_20_per_year.groupby(\"Year\").tail(1)[\"Year\"],\n",
    ")\n",
    "plt.ylabel(\"NDVI\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be13ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ndvi_df, prod_df):\n",
    "        self.ndvi_df = ndvi_df\n",
    "        self.prod_df = prod_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ndvi_df[\"Year\"].nunique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        years = self.ndvi_df[\"Year\"].sort_values().unique()\n",
    "        if idx >= len(years):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        year = years[idx]\n",
    "        ndvi = self.ndvi_df[self.ndvi_df[\"Year\"] == year][\"NDVI\"].values\n",
    "        prod = self.prod_df[self.prod_df[\"Year\"] == year][\n",
    "            \"Productivity (kg/ha)\"\n",
    "        ].values[0]\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = MLPDataset(NDVI_last_20_per_year, PROD)\n",
    "dataset[0]\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dateset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, valid_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b975",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62edc0",
   "metadata": {},
   "source": [
    "### 3.1. Multi-layer Perceptron\n",
    "\n",
    "Essa rede é uma feedforward perceptron multi-layer comum (1 camada interna).\n",
    "\n",
    "As entradas são os 20 últimos NDVIs do ano, a saída é a produtividade prevista (kg/ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4442be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = nn.Sequential(\n",
    "    nn.Linear(20, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ")\n",
    "mlp_network = mlp_network.to(device)\n",
    "optimizer = optim.Adam(mlp_network.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "best_loss = float(\"inf\")\n",
    "for i in range(100):\n",
    "    mlp_network.train()\n",
    "    for ndvi, prod in train_dataset:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = mlp_network(ndvi)\n",
    "        loss = loss_fn(pred, torch.tensor([prod], device=device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_losses = []\n",
    "    mlp_network.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in valid_dateset:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = mlp_network(ndvi)\n",
    "            loss = loss_fn(pred, torch.tensor([prod], device=device))\n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "        if np.mean(epoch_losses) < best_loss:\n",
    "            best_loss = np.mean(epoch_losses)\n",
    "            torch.save(mlp_network.state_dict(), \"mlp.pth\")\n",
    "            print(f\"Saving MLP model\\tepoch: {i+1}\\tvalidation loss: {best_loss:.4f}\")\n",
    "    \n",
    "    losses.append(np.mean(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs.\n",
    "\n",
    "    Args:\n",
    "        losses (list): List of loss values for each epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "plot_training_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bbc47",
   "metadata": {},
   "source": [
    "### 3.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df\n",
    "        self.prod_df = prod_df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ndvi_df) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ndvi = self.ndvi_df.iloc[idx : idx + self.window_size][\n",
    "            [\"NDVI\", \"Date_sin\", \"Date_cos\"]\n",
    "        ].values\n",
    "\n",
    "        year = self.ndvi_df.iloc[idx + self.window_size][\"Year\"]\n",
    "        prod = self.prod_df[self.prod_df[\"Year\"] == year][\n",
    "            \"Productivity (kg/ha)\"\n",
    "        ].values[0]\n",
    "\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "dataset = LSTMDataset(NDVI_last_20_per_year, PROD, LSTM_WINDOW_SIZE)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
    "valid_dateset = torch.utils.data.Subset(\n",
    "    dataset, range(train_size, train_size + valid_size)\n",
    ")\n",
    "test_dataset = torch.utils.data.Subset(\n",
    "    dataset, range(train_size + valid_size, train_size + valid_size + test_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define model with Linear layer\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=LSTM_DROPOUT\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, x, hidden_n=None, hidden_c=None):\n",
    "        if hidden_n is None or hidden_c is None:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out[:, -1, :])  # Get output of last time step\n",
    "        else:\n",
    "            out, (hidden_n, hidden_c) = self.lstm(x, (hidden_n, hidden_c))\n",
    "            out = self.fc(out[:, -1, :])  # Get output of last time step\n",
    "            return out, (hidden_n, hidden_c)\n",
    "\n",
    "\n",
    "lstm_model = LSTMRegressor(\n",
    "    input_size=3, hidden_size=LSTM_HIDDEN_SIZE, num_layers=LSTM_NUM_LAYERS\n",
    ").to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "lstm_losses = []\n",
    "best_loss = float(\"inf\")\n",
    "saved_epoch = 0\n",
    "\n",
    "\n",
    "for i in range(LSTM_EPOCHS):\n",
    "    h_n = torch.zeros(LSTM_NUM_LAYERS, 1, LSTM_HIDDEN_SIZE).to(device)  # Hidden state\n",
    "    h_c = torch.zeros(LSTM_NUM_LAYERS, 1, LSTM_HIDDEN_SIZE).to(device)  # Cell state\n",
    "\n",
    "    lstm_model.train()\n",
    "    for ndvi, prod in train_dataset:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred, (h_n, h_c) = lstm_model(\n",
    "            ndvi.unsqueeze(0), h_n.detach(), h_c.detach()\n",
    "        )  # Add batch dimension: [1, seq_len, 3]\n",
    "        # print(pred.shape)\n",
    "        last_pred = pred[:, -1]  # Get the last prediction\n",
    "        # print(last_pred.shape)\n",
    "        loss = loss_fn(last_pred, prod.unsqueeze(0))  # Match shape: [1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_losses = []\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in valid_dateset:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = lstm_model(ndvi.unsqueeze(0))\n",
    "            last_pred = pred[:, -1]  # Get the last prediction\n",
    "            loss = loss_fn(last_pred, prod.unsqueeze(0))\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            saved_epoch = i + 1\n",
    "            torch.save(lstm_model.state_dict(), \"lstm.pth\")\n",
    "\n",
    "    # if (i) % 10 == 0:\n",
    "    print(f\"Epoch {i+1}/{LSTM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    lstm_losses.append(avg_loss)\n",
    "\n",
    "print(f\"\\n\\nSaved LSTM model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(lstm_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
