{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376854e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df594f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_PATH = \"../data/PROCESSED/ndvi.csv\"\n",
    "PROD_PATH = \"../data/PROCESSED/manhuacu.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Hyperparameters\n",
    "MLP_WINDOW_SIZE = 5\n",
    "MLP_BATCH_SIZE = 4\n",
    "MLP_BASE_HIDDEN_SIZE = 32\n",
    "MLP_EPOCHS = 300\n",
    "MLP_LEARNING_RATE = 2e-3\n",
    "\n",
    "# Best MLP Hyperparameters\n",
    "# MLP_WINDOW_SIZE = 10\n",
    "# MLP_BATCH_SIZE = 4\n",
    "# MLP_BASE_HIDDEN_SIZE = 32\n",
    "# MLP_EPOCHS = 300\n",
    "# MLP_LEARNING_RATE = 2e-3\n",
    "\n",
    "# Validation Loss: 0.1911\n",
    "\n",
    "# LSTM Hyperparameters\n",
    "LSTM_WINDOW_SIZE = 20\n",
    "LSTM_HIDDEN_SIZE = 32\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_EPOCHS = 1000\n",
    "LSTM_BATCH_SIZE = 16\n",
    "LSTM_LEARNING_RATE = 5e-5\n",
    "\n",
    "# Best LSTM Hyperparameters So Far\n",
    "# LSTM_WINDOW_SIZE = 20\n",
    "# LSTM_HIDDEN_SIZE = 32\n",
    "# LSTM_NUM_LAYERS = 2\n",
    "# LSTM_DROPOUT = 0.2\n",
    "# LSTM_EPOCHS = 1000\n",
    "# LSTM_BATCH_SIZE = 16\n",
    "# LSTM_LEARNING_RATE = 5e-5\n",
    "\n",
    "# Validation Loss: 0.2122\n",
    "\n",
    "# Computation\n",
    "LSTM_DROPOUT = LSTM_DROPOUT if LSTM_NUM_LAYERS > 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_year_index(date: datetime):\n",
    "    \"\"\"Convert date to day of year.\"\"\"\n",
    "    return datetime(date.year, date.month, date.day).timetuple().tm_yday - 1\n",
    "\n",
    "\n",
    "def get_sin_cos(x: float):\n",
    "    \"\"\"Convert x to sin and cos.\"\"\"\n",
    "    rad = 2 * np.pi * x\n",
    "    return (np.sin(rad), np.cos(rad))\n",
    "\n",
    "\n",
    "def encode_date(date: datetime):\n",
    "    is_leap_year = 1 if date.year % 4 == 0 else 0\n",
    "    total_year_days = 366 if is_leap_year else 365\n",
    "    day_index = get_day_of_year_index(date)\n",
    "    return get_sin_cos(day_index / total_year_days)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Encoding date 2020-01-01\")\n",
    "print(encode_date(datetime(2020, 1, 1)))  # (0.0, 1.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-06-01\")\n",
    "print(encode_date(datetime(2020, 6, 1)))  # (0.5, 0.0)\n",
    "print(\"\\n\")\n",
    "print(\"Encoding date 2020-12-31\")\n",
    "print(encode_date(datetime(2020, 12, 31)))  # (0.9999999999999999, 1.0)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ca232",
   "metadata": {},
   "source": [
    "## 1. Carregar e Pré-processar Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fba421",
   "metadata": {},
   "source": [
    "### 1.1. Carregar e pre-processar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI = pd.read_csv(NDVI_PATH)\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI[\"N_Observations\"] = NDVI.groupby(\"Year\")[\"Data\"].transform(\"count\")\n",
    "\n",
    "NDVI[[\"Date_sin\", \"Date_cos\"]] = NDVI[\"Data\"].apply(\n",
    "    lambda x: pd.Series(encode_date(datetime.strptime(x, \"%Y-%m-%d\")))\n",
    ")\n",
    "\n",
    "# Assert order by Data (ascending)\n",
    "NDVI = NDVI.sort_values(by=\"Data\", ascending=True)\n",
    "\n",
    "NDVI = NDVI[(NDVI[\"Year\"] >= 2000) & (NDVI[\"Year\"] <= 2023)]\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = pd.read_csv(PROD_PATH)\n",
    "PROD = PROD[(PROD[\"Year\"] >= 2000) & (PROD[\"Year\"] <= 2023)]\n",
    "# max_productivity = PROD[\"Productivity (kg/ha)\"].max()\n",
    "# PROD[\"Normalized_productivity\"] = PROD[\"Productivity (kg/ha)\"] / max_productivity\n",
    "PROD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba00741",
   "metadata": {},
   "source": [
    "### 1.2. Visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e981475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.plot(x=\"Data\", y=\"NDVI\", title=\"NDVI over time\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be13ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLPDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, ndvi_df, prod_df):\n",
    "#         self.ndvi_df = ndvi_df\n",
    "#         self.prod_df = prod_df\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.ndvi_df[\"Year\"].nunique()\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         years = self.ndvi_df[\"Year\"].sort_values().unique()\n",
    "#         if idx >= len(years):\n",
    "#             raise IndexError(\"Index out of range\")\n",
    "#         year = years[idx]\n",
    "#         ndvi = self.ndvi_df[self.ndvi_df[\"Year\"] == year][\"NDVI\"].values\n",
    "#         prod = self.prod_df[self.prod_df[\"Year\"] == year][\n",
    "#             \"Productivity (kg/ha)\"\n",
    "#         ].values[0]\n",
    "#         return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "#             prod, dtype=torch.float32\n",
    "#         )\n",
    "\n",
    "\n",
    "# dataset = MLPDataset(NDVI_last_20_per_year, PROD)\n",
    "# dataset[0]\n",
    "\n",
    "# train_size = len(dataset) - 8\n",
    "# valid_size = 4\n",
    "# test_size = 4\n",
    "# train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
    "# valid_dateset = torch.utils.data.Subset(dataset, range(train_size, train_size + valid_size))\n",
    "# test_dataset = torch.utils.data.Subset(dataset, range(train_size + valid_size, train_size + valid_size + test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb013d",
   "metadata": {},
   "source": [
    "### 2.2. Preparar Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbdd8",
   "metadata": {},
   "source": [
    "#### 2.1.1. Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizer dados NDVI\n",
    "NDVI[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "ndvi_scaler = StandardScaler().fit(NDVI[[\"NDVI\", \"Year\"]].values)\n",
    "NDVI[[\"NDVI_norm\", \"Year_norm\"]] = ndvi_scaler.transform(NDVI[[\"NDVI\", \"Year\"]].values)\n",
    "\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd382cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar produtividade\n",
    "PROD[\"Year_norm\"] = NDVI[\"Year\"].copy()\n",
    "\n",
    "prod_scaler = StandardScaler().fit(PROD[[\"Productivity (kg/ha)\", \"Year\"]].values)\n",
    "PROD[[\"Productivity_norm\", \"Year_norm\"]] = prod_scaler.transform(\n",
    "    PROD[[\"Productivity (kg/ha)\", \"Year\"]].values\n",
    ")\n",
    "PROD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DatasetYearOfLast(torch.utils.data.Dataset):\n",
    "#     \"\"\"\n",
    "#     DatasetYearOfLast - Dataset para previsão de produtividade\n",
    "\n",
    "#     X: Sequências de tamanho <WINDOW_SIZE> de observações de NDVI consecutivas (normalizado -1 a +1)\n",
    "#     y: Produtividade no Ano da última observação\n",
    "\n",
    "#     Features:\n",
    "#     - Sequências de NDVI (Já vem normalizado entre 0 e 1 da fonte)\n",
    "#     - Sequências de dia do ano com codificação circular no formato de Tupla: (Seno, Cosseno)\n",
    "#     - Sequência de ano da observação normalizado por z-score\n",
    "\n",
    "#     Label:\n",
    "#     - Produtividade (kg/ha) normalizada por z-score, relativa ao ano da última observação\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "#         self.ndvi_df = ndvi_df\n",
    "#         self.prod_df = prod_df\n",
    "#         self.window_size = window_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ndvi_df) - self.window_size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         ndvi = self.ndvi_df.iloc[idx : idx + self.window_size][\n",
    "#             [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "#         ].values\n",
    "\n",
    "#         year = self.ndvi_df.iloc[idx + self.window_size - 1][\"Year\"]\n",
    "#         prod = self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0]\n",
    "\n",
    "#         return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "#             prod, dtype=torch.float32\n",
    "#         )\n",
    "\n",
    "\n",
    "class DatasetYearOfLast(torch.utils.data.Dataset):\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df.copy().reset_index(drop=True)\n",
    "        self.window_size = window_size\n",
    "        self.prod_df = prod_df\n",
    "\n",
    "        # Prepare windows grouped by year\n",
    "        self.samples = []\n",
    "        self.available_years = ndvi_df[\"Year\"].unique().tolist()\n",
    "\n",
    "        for idx, row in self.ndvi_df.iterrows():\n",
    "            window = ndvi_df.iloc[idx : idx + window_size]\n",
    "            \n",
    "            if len(window) < window_size:\n",
    "                break\n",
    "            \n",
    "            last_of_window = self.ndvi_df.iloc[idx + window_size - 1]\n",
    "            \n",
    "\n",
    "            if last_of_window[\"Year\"] == row[\"Year\"] or (\n",
    "                last_of_window[\"Year\"] == row[\"Year\"] + 1\n",
    "                and row[\"Year\"] + 1 in self.available_years\n",
    "            ):\n",
    "                self.samples.append((window, last_of_window[\"Year\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window, year = self.samples[idx]\n",
    "        ndvi = window[[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]].values\n",
    "\n",
    "        prod = self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0]\n",
    "\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def get_last_window_of_year(self, year) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retorna a última janela do ano\n",
    "        \"\"\"\n",
    "\n",
    "        ndvi = self.ndvi_df[self.ndvi_df[\"Year\"] == year]\n",
    "        if len(ndvi) < self.window_size:\n",
    "            raise ValueError(f\"Year {year} not found in dataset\")\n",
    "        return (\n",
    "            ndvi.iloc[-self.window_size :][\n",
    "                [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "            ].values,\n",
    "            self.prod_df[self.prod_df[\"Year\"] == year][\"Productivity_norm\"].values[0],\n",
    "        )\n",
    "\n",
    "years_validation = [2004, 2010, 2016, 2022]\n",
    "years_test = [2005, 2011, 2017, 2023]\n",
    "years_train = PROD[~PROD[\"Year\"].isin(years_validation + years_test)][\"Year\"].unique()\n",
    "\n",
    "\n",
    "# Datasets better\n",
    "mlp_train_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_train)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "mlp_validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "mlp_test_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, MLP_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "lstm_train_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_train)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "lstm_validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_validation)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "lstm_test_dataset_year_of_last = DatasetYearOfLast(\n",
    "    NDVI[NDVI[\"Year\"].isin(years_test)], PROD, LSTM_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# Datasets sequential\n",
    "# train_dataset_year_of_last = DatasetYearOfLast(\n",
    "#     NDVI[NDVI[\"Year\"] <= 2016], PROD, LSTM_WINDOW_SIZE\n",
    "# )\n",
    "# validation_dataset_year_of_last = DatasetYearOfLast(\n",
    "#     NDVI[(NDVI[\"Year\"] > 2016) & (NDVI[\"Year\"] <= 2020)], PROD, LSTM_WINDOW_SIZE\n",
    "# )\n",
    "# test_dataset_year_of_last = DatasetYearOfLast(\n",
    "#     NDVI[NDVI[\"Year\"] > 2020], PROD, LSTM_WINDOW_SIZE\n",
    "# )\n",
    "\n",
    "# print(mlp_validation_dataset_year_of_last[0][0].shape)\n",
    "print(mlp_validation_dataset_year_of_last[0])\n",
    "print(\"\\n\")\n",
    "print(mlp_validation_dataset_year_of_last.get_last_window_of_year(2004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a26fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"Last window of 2017 in the DataFrame:\")\n",
    "print(\n",
    "    NDVI[NDVI[\"Year\"] == 2017][[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]].tail(\n",
    "        LSTM_WINDOW_SIZE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nProdutivity 2017: {PROD[PROD['Year'] == 2017]['Productivity (kg/ha)'].values[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Produtivity 2017 (normalized): {PROD[PROD['Year'] == 2017]['Productivity_norm'].values[0]}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nLast window of 2017 in the DatasetYearOfLast (values should match exactely):\")\n",
    "print(lstm_test_dataset_year_of_last.get_last_window_of_year(2017))\n",
    "\n",
    "assert (\n",
    "    lstm_test_dataset_year_of_last.get_last_window_of_year(2017)[0]\n",
    "    == NDVI[NDVI[\"Year\"] == 2017][[\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]]\n",
    "    .tail(LSTM_WINDOW_SIZE)\n",
    "    .values\n",
    ").all(), \"\\n❌ Sanity check failed! Please check the DatasetYearOfLast class\"\n",
    "print(\n",
    "    \"\\n✅ Sanity check passed for LSTM! You can look values by yourself if you want to double check.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ae9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWeightedAverage(torch.utils.data.Dataset):\n",
    "    \"\"\"DatasetWeightedAverage - Dataset para previsão de produtividade\n",
    "\n",
    "    X: Sequências de tamanho <WINDOW_SIZE> de observações de NDVI consecutivas (normalizado -1 a +1)\n",
    "    y: Produtividade média ponderada entre a produtividade do ano da primeira observação e do ano da última observação\n",
    "    - (normalizado por z-score)\n",
    "    - A média é ponderada pela quantidade de observações do ano da primeira e do ano da última observação\n",
    "\n",
    "    Features:\n",
    "    - Sequências de NDVI (Já vem normalizado entre 0 e 1 da fonte)\n",
    "    - Sequências de dia do ano com codificação circular no formato de Tupla: (Seno, Cosseno)\n",
    "    - Sequência de ano da observação normalizado por z-score\n",
    "\n",
    "    Label:\n",
    "    - Produtividade (kg/ha) média ponderada entre o ano da primeira e do ano da última observação, normalizada por z-score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndvi_df, prod_df, window_size=LSTM_WINDOW_SIZE):\n",
    "        self.ndvi_df = ndvi_df\n",
    "        self.prod_df = prod_df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ndvi_df) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ndvi = self.ndvi_df.iloc[idx : idx + self.window_size][\n",
    "            [\"NDVI\", \"Date_sin\", \"Date_cos\", \"Year_norm\"]\n",
    "        ].values\n",
    "\n",
    "        year_first = self.ndvi_df.iloc[idx][\"Year\"]\n",
    "        year_last = self.ndvi_df.iloc[idx + self.window_size][\"Year\"]\n",
    "\n",
    "        prod_first = self.prod_df[self.prod_df[\"Year\"] == year_first][\n",
    "            \"Productivity_norm\"\n",
    "        ].values[0]\n",
    "        prod_last = self.prod_df[self.prod_df[\"Year\"] == year_last][\n",
    "            \"Productivity_norm\"\n",
    "        ].values[0]\n",
    "\n",
    "        n_obs_first = self.ndvi_df.iloc[idx : idx + self.window_size].loc[\n",
    "            self.ndvi_df.iloc[idx : idx + self.window_size][\"Year\"] == year_first\n",
    "        ].shape[0]\n",
    "        n_obs_last = self.ndvi_df.iloc[idx : idx + self.window_size].loc[\n",
    "            self.ndvi_df[\"Year\"] == year_last\n",
    "        ].shape[0]\n",
    "\n",
    "        # Ponderação\n",
    "        prod = (n_obs_first * prod_first + n_obs_last * prod_last) / (\n",
    "            n_obs_first + n_obs_last\n",
    "        )\n",
    "        return torch.tensor(ndvi, dtype=torch.float32), torch.tensor(\n",
    "            prod, dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "# Test Dataset\n",
    "train_dataset_year_of_last = DatasetWeightedAverage(NDVI[NDVI[\"Year\"] <= 2016], PROD, LSTM_WINDOW_SIZE)\n",
    "validation_dataset_year_of_last = DatasetWeightedAverage(NDVI[(NDVI[\"Year\"] > 2016) & (NDVI[\"Year\"] <= 2020)], PROD, LSTM_WINDOW_SIZE)\n",
    "test_dataset_weighted_average = DatasetWeightedAverage(NDVI[NDVI[\"Year\"] > 2020], PROD, LSTM_WINDOW_SIZE)\n",
    "\n",
    "validation_dataset_year_of_last[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b975",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62edc0",
   "metadata": {},
   "source": [
    "### 3.1. Multi-layer Perceptron\n",
    "\n",
    "Essa rede é uma feedforward perceptron multi-layer comum (1 camada interna).\n",
    "\n",
    "As entradas são os 20 últimos NDVIs do ano, a saída é a produtividade prevista (kg/ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4442be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmplifiedTanh(nn.Module):\n",
    "    def __init__(self, amplification_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.amplification_factor = amplification_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.amplification_factor * torch.tanh(x)\n",
    "\n",
    "\n",
    "mlp_network = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(MLP_WINDOW_SIZE * 4, MLP_BASE_HIDDEN_SIZE),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(MLP_BASE_HIDDEN_SIZE, MLP_BASE_HIDDEN_SIZE // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(MLP_BASE_HIDDEN_SIZE // 2, 1),\n",
    "    AmplifiedTanh(amplification_factor=1.5),\n",
    ")\n",
    "\n",
    "\n",
    "def init_linear_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.normal_(m.bias, mean=0.0, std=0.01)\n",
    "\n",
    "\n",
    "mlp_network.apply(init_linear_weights)\n",
    "\n",
    "# for name, param in mlp_network.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n",
    "\n",
    "# Step-by-step debug the MLP\n",
    "# x = torch.randn(20, 4)\n",
    "# print(f\"Input shape: {x.shape}\\n{x}\\n\")\n",
    "# for i, layer in enumerate(mlp_network):\n",
    "#     x = layer(x)\n",
    "#     print(f\"After layer {i} ({layer.__class__.__name__}): {x.shape}\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = mlp_network.to(device)\n",
    "optimizer = optim.Adam(mlp_network.parameters(), lr=MLP_LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "mlp_losses_validation = []\n",
    "mlp_losses_train = []\n",
    "best_loss = float(\"inf\")\n",
    "saved_epoch = 0\n",
    "\n",
    "mlp_train_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    mlp_train_dataset_year_of_last,\n",
    "    batch_size=MLP_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "mlp_validation_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    mlp_validation_dataset_year_of_last, batch_size=4, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "for i in trange(MLP_EPOCHS):\n",
    "    epoch_losses_train = []\n",
    "\n",
    "    mlp_network.train()\n",
    "    for ndvi, prod in mlp_train_loader_year_of_last:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = mlp_network(ndvi)\n",
    "        loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    epoch_losses_validation = []\n",
    "    mlp_network.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in mlp_validation_loader_year_of_last:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = mlp_network(ndvi)\n",
    "            loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "            epoch_losses_validation.append(loss.item())\n",
    "\n",
    "        if np.mean(epoch_losses_validation) < best_loss:\n",
    "            best_loss = np.mean(epoch_losses_validation)\n",
    "            saved_epoch = i + 1\n",
    "            torch.save(mlp_network.state_dict(), \"mlp.pth\")\n",
    "\n",
    "    mlp_losses_train.append(np.mean(epoch_losses_train))\n",
    "    mlp_losses_validation.append(np.mean(epoch_losses_validation))\n",
    "print(f\"\\n\\nSaved MLP model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, validation_losses):\n",
    "    \"\"\"\n",
    "    Plots the training and validation losses over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training loss values for each epoch.\n",
    "        validation_losses (list): List of validation loss values for each epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(mlp_losses_train, mlp_losses_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bbc47",
   "metadata": {},
   "source": [
    "### 3.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "def cat_name_generator(gender: str): return random.choice({\"macho\": [\"Thor\", \"Loki\", \"Zeus\", \"Hades\", \"Apolo\", \"Ares\", \"Hermes\", \"Poseidon\", \"Hércules\", \"Aquiles\", \"Ulisses\", \"Atlas\", \"Perseu\", \"Orfeu\", \"Eros\", \"Hefesto\", \"Dionísio\", \"Héracles\", \"Cronos\", \"Prometeu\", \"Teseu\", \"Orfeu\", \"Eolo\"], \"femea\": [\"Afrodite\", \"Artemis\", \"Deméter\", \"Hera\", \"Perséfone\", \"Atena\", \"Héstia\", \"Eris\", \"Selene\", \"Gaia\", \"Tétis\", \"Eurídice\", \"Calipso\", \"Medusa\", \"Circe\"]}[gender])\n",
    "\n",
    "print(\"Nome de gato macho:\", cat_name_generator(\"macho\"))\n",
    "print(\"Nome de gato fêmea:\", cat_name_generator(\"femea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define model with Linear layer\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=LSTM_DROPOUT\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, x, hidden_n=None, hidden_c=None):\n",
    "        if hidden_n is None or hidden_c is None:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out[:, -1, :])  # Get output of last time step\n",
    "        else:\n",
    "            out, (hidden_n, hidden_c) = self.lstm(x, (hidden_n, hidden_c))\n",
    "            out = self.fc(out[:, -1, :])  # Get output of last time step\n",
    "            return out, (hidden_n, hidden_c)\n",
    "\n",
    "\n",
    "lstm_train_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    lstm_train_dataset_year_of_last,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "lstm_validation_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    lstm_validation_dataset_year_of_last,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "\n",
    "lstm_model = LSTMRegressor(\n",
    "    input_size=4, hidden_size=LSTM_HIDDEN_SIZE, num_layers=LSTM_NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "# for name, param in lstm_model.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n",
    "\n",
    "\n",
    "def init_lstm_weights(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        nn.init.xavier_uniform_(m.weight_ih_l0)\n",
    "        nn.init.xavier_uniform_(m.weight_hh_l0)\n",
    "\n",
    "\n",
    "lstm_model.apply(init_lstm_weights)\n",
    "lstm_model.apply(init_linear_weights)\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=LSTM_LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "lstm_losses_train = []\n",
    "lstm_losses_validation = []\n",
    "best_loss = float(\"inf\")\n",
    "saved_epoch = 0\n",
    "\n",
    "\n",
    "for i in trange(LSTM_EPOCHS):\n",
    "    epoch_losses_train = []\n",
    "\n",
    "    # h_n = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "    #     device\n",
    "    # )  # Hidden state\n",
    "    # h_c = torch.zeros(LSTM_NUM_LAYERS, LSTM_BATCH_SIZE, LSTM_HIDDEN_SIZE).to(\n",
    "    #     device\n",
    "    # )  # Cell state\n",
    "\n",
    "    lstm_model.train()\n",
    "    for ndvi, prod in lstm_train_loader_year_of_last:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # pred, (h_n, h_c) = lstm_model(\n",
    "        #     ndvi, h_n.detach(), h_c.detach()\n",
    "        # )\n",
    "        \n",
    "        # Verificar se isto está certo\n",
    "        # last_pred está correto?\n",
    "        pred = lstm_model(ndvi)\n",
    "        last_pred = pred[:, -1]\n",
    "        loss = loss_fn(last_pred, prod)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    epoch_losses_validation = []\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ndvi, prod in lstm_validation_loader_year_of_last:\n",
    "            ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "            pred = lstm_model(ndvi)\n",
    "            last_pred = pred[:, -1]  # Get the last prediction\n",
    "            loss = loss_fn(last_pred, prod)\n",
    "            epoch_losses_validation.append(loss.item())\n",
    "\n",
    "        if np.mean(epoch_losses_validation) < best_loss:\n",
    "            best_loss = np.mean(epoch_losses_validation)\n",
    "            saved_epoch = i + 1\n",
    "            torch.save(lstm_model.state_dict(), \"lstm.pth\")\n",
    "\n",
    "    lstm_losses_train.append(np.mean(epoch_losses_train))\n",
    "    lstm_losses_validation.append(np.mean(epoch_losses_validation))\n",
    "\n",
    "print(f\"\\n\\nSaved LSTM model\\tepoch: {saved_epoch}\\tvalidation loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(lstm_losses_train, lstm_losses_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae30be1",
   "metadata": {},
   "source": [
    "## 4. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    mlp_test_dataset_year_of_last, batch_size=4, shuffle=False, drop_last=True\n",
    ")\n",
    "lstm_test_loader_year_of_last = torch.utils.data.DataLoader(\n",
    "    lstm_test_dataset_year_of_last, batch_size=4, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# test MLP\n",
    "test_losses_mlp = []\n",
    "mlp_network.eval()\n",
    "for ndvi, prod in mlp_test_loader_year_of_last:\n",
    "    ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "    pred = mlp_network(ndvi)\n",
    "    loss = loss_fn(pred, prod.unsqueeze(1))\n",
    "    test_losses_mlp.append(loss.item())\n",
    "print(f\"MLP test loss: {np.mean(test_losses_mlp):.4f}\")\n",
    "\n",
    "# test LSTM\n",
    "test_losses_lstm = []\n",
    "lstm_model.eval()\n",
    "for ndvi, prod in lstm_test_loader_year_of_last:\n",
    "    ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "    pred = lstm_model(ndvi)\n",
    "    last_pred = pred[:, -1]  # Get the last prediction\n",
    "    loss = loss_fn(last_pred, prod)\n",
    "    test_losses_lstm.append(loss.item())\n",
    "print(f\"LSTM test loss: {np.mean(test_losses_lstm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network.eval()\n",
    "all_preds = []\n",
    "real_prods = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ndvi, prod in mlp_test_loader_year_of_last:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        pred = mlp_network(ndvi)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        real_prods.append(prod.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and real productions\n",
    "all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "real_prods = np.concatenate(real_prods, axis=0).flatten()\n",
    "\n",
    "# Print predictions and real productions\n",
    "# print(\"LSTM Predictions for all test dataset:\")\n",
    "# print(all_preds)\n",
    "# print(\"\\nReal Production (normalized) for all test dataset:\")\n",
    "# print(real_prods)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(real_prods, label=\"Real Production (normalized)\", marker='o')\n",
    "plt.plot(all_preds, label=\"MLP Predictions (normalized)\", marker='x')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Normalized Values\")\n",
    "plt.title(\"MLP Predictions vs Real Production\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56981a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "all_preds = []\n",
    "real_prods = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ndvi, prod in lstm_test_loader_year_of_last:\n",
    "        ndvi, prod = ndvi.to(device), prod.to(device)\n",
    "        pred = lstm_model(ndvi)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        real_prods.append(prod.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and real productions\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "real_prods = np.concatenate(real_prods, axis=0)\n",
    "\n",
    "# Print predictions and real productions\n",
    "# print(\"LSTM Predictions for all test dataset:\")\n",
    "# print(all_preds)\n",
    "# print(\"\\nReal Production (normalized) for all test dataset:\")\n",
    "# print(real_prods)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(real_prods, label=\"Real Production (normalized)\", marker='o')\n",
    "plt.plot(all_preds, label=\"LSTM Predictions (normalized)\", marker='x')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Normalized Values\")\n",
    "plt.title(\"LSTM Predictions vs Real Production\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP test loss:\", np.mean(test_losses_mlp))\n",
    "for y_test in years_test:\n",
    "    ndvi, prod = mlp_test_dataset_year_of_last.get_last_window_of_year(y_test)\n",
    "    ndvi = torch.tensor(ndvi, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = mlp_network(ndvi)\n",
    "    last_pred = pred[:, -1].cpu().detach().numpy()[0]\n",
    "    test_losses_mlp.append(last_pred)\n",
    "    print(f\"\\n[MLP] Predicted (normalized) productivity for {y_test}: {last_pred:.4f}\")\n",
    "    print(f\"[MLP] Real productivity (normalized) for {y_test}: {prod:.4f}\")\n",
    "\n",
    "print(\"\\n\\nLSTM test loss:\", np.mean(test_losses_lstm))\n",
    "for y_test in years_test:\n",
    "    ndvi, prod = lstm_test_dataset_year_of_last.get_last_window_of_year(y_test)\n",
    "    ndvi = torch.tensor(ndvi, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    pred = lstm_model(ndvi)\n",
    "    last_pred = pred[:, -1].cpu().detach().numpy()[0]\n",
    "    print(f\"\\n[LSTM] Predicted productivity (normalized) for {y_test}: {last_pred:.4f}\")\n",
    "    print(f\"[LSTM] Real productivity (normalized) for {y_test}: {prod:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
